{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65affba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary packages\n",
    "import pandas as pd\n",
    "import hypernetx as hnx\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, fpmax\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148746f",
   "metadata": {},
   "source": [
    "# Step 1. Generating Transactions from Mobility Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbaea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the mobility dataframe with movements of 100,000 individuals over 75-day business as usual period\n",
    "dataset_path = 'YJMob100k/yjmob100k-dataset1.csv' \n",
    "# The original dataset has 200x200 grid (area of a grid cell = 1/2 km x1/2 km = 0.25 km^2)\n",
    "# Scaling_factor 10 (200/10 x 200/10) aggregates it to a 20x20 grid (area of a grid cell = 5 km x 5 km = 25 km^2)\n",
    "scaling_factor = 10\n",
    "# List of minimum support thresholds considered [1%, 1.5%, 2%]\n",
    "min_sups = [0.005, 0.01, 0.015] #minimum support values corresponding to 1% and 1.5% respectively\n",
    "deltaT_vals = [1, 3, 7] #Sliding window lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_set_of_transactions_by_deltaT(dataset_path, scaling_factor, deltaT):\n",
    "    mob_df = pd.read_csv(dataset_path)\n",
    "    print(\"Delta T = \"+str(deltaT))\n",
    "    \n",
    "    #Spatial aggregation of 200x200 grid to 20x20 grid\n",
    "    mob_df['grouped_x'] = (mob_df['x'] - 1) // scaling_factor + 1\n",
    "    mob_df['grouped_y'] = (mob_df['y'] - 1) // scaling_factor + 1   \n",
    "    \n",
    "    #start and end days in the dataset\n",
    "    min_day = mob_df['d'].min()\n",
    "    max_day = mob_df['d'].max()\n",
    "    print(min_day, max_day)\n",
    "    \n",
    "    #sliding window of deltaT length over min_day and max_day to obtain transactions\n",
    "    transactions = []\n",
    "    for start_day in range(min_day, max_day - deltaT + 2):\n",
    "        end_day = start_day + deltaT - 1\n",
    "        window_df = mob_df[ (mob_df['d'] >= start_day) & (mob_df['d'] <= end_day) ]\n",
    "        mob_df_grouped = window_df.groupby('uid')\n",
    "    \n",
    "        if start_day % 10 == 0:\n",
    "            print(start_day)\n",
    "        for uid, record in mob_df_grouped:\n",
    "            transaction = frozenset(zip(record['grouped_x'], record['grouped_y']))\n",
    "            transactions.append(transaction)\n",
    "\n",
    "    filename = f\"transactions_scaling_{scaling_factor}_deltaT_{deltaT}.txt\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        for transaction in transactions:\n",
    "            transaction_str = \"; \".join(map(str, transaction))  # Convert frozenset to a string\n",
    "            f.write(transaction_str + \"\\n\")\n",
    "    \n",
    "    print(f\"A total of {len(transactions)} transactions saved to {filename}\")\n",
    "    \n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for deltaT in deltaT_vals:\n",
    "    transactions_per_deltaT = generate_set_of_transactions_by_deltaT(dataset_path, scaling_factor, deltaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31142da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transactions_from_file(filename):\n",
    "    transactions = []\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Strip newline and split by commas to get pairs as strings\n",
    "            pairs = line.strip().split(\"; \")\n",
    "            \n",
    "            # Convert each pair from string to a tuple of integers\n",
    "            transaction = frozenset(\n",
    "                tuple(map(int, pair.strip(\"()\").split(\",\"))) for pair in pairs\n",
    "            )\n",
    "            transactions.append(transaction)\n",
    "    \n",
    "    print(f\"Loaded {len(transactions)} transactions from {filename}\")\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa726b9",
   "metadata": {},
   "source": [
    "# Step 2. Computing Frequent Itemsets from Transactions Using FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes and returns all frequent itemsets given min_support and threshold. min_support is the \n",
    "# minimum fraction of total transactions in which the itemset appears; Set min_support to a suitable \n",
    "#Â threshold (for instance, 0.05 is 5%, 0.01 is 1%, 0.005 is 0.5%, 0.001 is 0.1%). min_itemset_size is the\n",
    "# minimum number of items in any itemset. By setting it to 2, we get frequent itemsets which are of size \n",
    "# at least 2 (two or more locations are visited in the itemset).\n",
    "def get_frequent_itemsets(transactions, min_sup, min_itemset_size):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)  \n",
    "    frequent_itemsets = fpgrowth(df, min_support=min_sup, use_colnames=True)\n",
    "    print(\"Total frequent itemsets: \", len(frequent_itemsets))\n",
    "    frequent_itemsets_filtered = frequent_itemsets[\n",
    "        frequent_itemsets['itemsets'].apply(lambda x: len(x) >= min_itemset_size)\n",
    "    ]\n",
    "    print(\"Filtered frequent itemsets: \", len(frequent_itemsets_filtered)) \n",
    "    print(\"minimum support, no. of frequent itemsets: \", min_sup, len(frequent_itemsets_filtered))\n",
    "    return frequent_itemsets_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff73759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_frequent_itemsets(frequent_itemsets, min_sup):\n",
    "    frequent_itemsets_filtered = frequent_itemsets[\n",
    "        frequent_itemsets['support'] >= min_sup\n",
    "    ]\n",
    "    print(\"minimum support, no. of frequent itemsets: \", min_sup, len(frequent_itemsets_filtered))\n",
    "    return frequent_itemsets_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_all_freq_itemsets_by_minsup_and_deltaT(transactions, deltaT, min_sups):\n",
    "    print(\"Minimum support values:\", min_sups)\n",
    "    \n",
    "    # Initialize a dictionary to store all itemsets by min_sup for reference or further processing, if needed.\n",
    "    freq_itemsets_by_minsup = {}\n",
    "\n",
    "    for i, min_sup in enumerate(min_sups):\n",
    "        print(f\"Processing min_sup={min_sup}\")\n",
    "        \n",
    "        # Generate or filter frequent itemsets based on min_sup value\n",
    "        if i == 0:\n",
    "            freq_itemsets = get_frequent_itemsets(transactions, min_sup, 2)\n",
    "        else:\n",
    "            freq_itemsets = get_filtered_frequent_itemsets(freq_itemsets_by_minsup[min_sups[0]], min_sup)\n",
    "        \n",
    "        # Store the itemsets in dictionary for each min_sup value\n",
    "        freq_itemsets_by_minsup[min_sup] = freq_itemsets\n",
    "\n",
    "        # Convert frequent itemsets to a DataFrame\n",
    "        freq_itemsets_df = pd.DataFrame({\n",
    "            'itemsets': freq_itemsets['itemsets'],\n",
    "            'support': freq_itemsets['support']\n",
    "        })\n",
    "\n",
    "        filename = f\"freq_itemsets_deltaT_{deltaT}_minsup_{min_sup}.csv\"\n",
    "        print(f\"Writing itemsets to {filename}\")\n",
    "\n",
    "        freq_itemsets_df.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"All frequent itemsets have been written to CSV files for {deltaT} and {min_sups}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(deltaT_vals)):\n",
    "    deltaT = deltaT_vals[i]\n",
    "    filename = f\"transactions_scaling_{scaling_factor}_deltaT_{deltaT}.txt\"\n",
    "    transactions = read_transactions_from_file(filename)\n",
    "    print(f\"Number of transactions: {len(transactions)}\")\n",
    "    write_all_freq_itemsets_by_minsup_and_deltaT(transactions, deltaT, min_sups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d39ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_freq_itemsets(deltaT, min_sup):\n",
    "    # Define the filename based on deltaT and min_sup values\n",
    "    filename = f\"outputs/DS1/freq_itemsets_deltaT_{deltaT}_minsup_{min_sup}.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        freq_itemsets_df = pd.read_csv(filename)\n",
    "        print(f\"Successfully loaded {filename}\")\n",
    "        \n",
    "        # Convert 'itemsets' column entries from frozenset string format to normal set\n",
    "        def parse_to_set(item):\n",
    "            # Remove 'frozenset({' and '})' to get the string representation of the set's contents\n",
    "            item = item.strip(\"frozenset({})\")\n",
    "            \n",
    "            # Now, split the string by '), (' to separate each tuple, then re-parse each tuple\n",
    "            items = item.split(\"), (\")\n",
    "            \n",
    "            # Convert each string tuple into an actual tuple and collect them into a set\n",
    "            item_set = set()\n",
    "            for i in items:\n",
    "                # Convert each string (e.g., '13, 9') into a tuple (13, 9)\n",
    "                item_set.add(tuple(map(int, i.split(\", \"))))\n",
    "            \n",
    "            return item_set\n",
    "\n",
    "        # Apply the parsing function to the 'itemsets' column\n",
    "        freq_itemsets_df['itemsets'] = freq_itemsets_df['itemsets'].apply(parse_to_set)\n",
    "\n",
    "        return freq_itemsets_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {filename} does not exist.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "freq_itemsets = read_freq_itemsets(1, 0.015)\n",
    "freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb3004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
